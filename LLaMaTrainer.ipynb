{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "343ffa9f-b0af-4ab2-8ed1-cdd6d015ae20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q -U bitsandbytes\n",
    "!pip install -q -U git+https://github.com/huggingface/transformers.git \n",
    "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "#pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "#current version of Accelerate on GitHub breaks QLoRa\n",
    "#Using standard pip instead\n",
    "!pip install -q -U accelerate\n",
    "!pip install -q -U datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf927889-0d80-48aa-8473-9c32218e9223",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27ac89c9-4a05-425a-ba91-2011d25d1620",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#model_name = \"Birchlabs/mosaicml-mpt-7b-chat-qlora\"\n",
    "model_name = \"huggyllama/llama-7b\"\n",
    "\n",
    "#Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c40149b-d57e-4555-8972-f182a6f356d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c148db13-0d7c-42a0-af31-d479319a4520",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting einops\n",
      "  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: einops\n",
      "Successfully installed einops-0.6.1\n"
     ]
    }
   ],
   "source": [
    "!pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9693e5db-5f45-45a6-97a0-d1cba5bd44ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4efc3a36da1469fa2fdd0db1590bd8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/594 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\n",
      "CUDA SETUP: CUDA runtime path found: /home/ec2-user/anaconda3/envs/pytorch_p310/lib/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 7.5\n",
      "CUDA SETUP: Detected CUDA version 118\n",
      "CUDA SETUP: Loading binary /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e3f84cc943e41fd8a8af7162c144d0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)fetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38a95a7e11c441a7a1a72d246ce0d255",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9d9999e32b04cd8881a7bd727a7669c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3412ee53c56d4badacd5c54c677b532b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45d401d82be64713a2ea8b9d729aca76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e68bf2834e7740f6bed5c152ada7e5c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=quant_config, device_map={\"\":0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2c6f518-e4a4-4a33-80cd-397db409d7b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1b21047-fae7-4775-ac67-93a83c6c8f1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=8, \n",
    "    lora_alpha=32, \n",
    "    target_modules=[\"q_proj\", \"v_proj\"], \n",
    "    lora_dropout=0.05, \n",
    "    bias=\"none\", \n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1aa37832-4e21-423a-893a-d9a93c26434b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset json/default to /home/ec2-user/.cache/huggingface/datasets/json/default-35d4744e250cc41e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5544836c0e748faaeed333225f99bf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1826a9dc62734fa98d20943843ebcbc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset json downloaded and prepared to /home/ec2-user/.cache/huggingface/datasets/json/default-35d4744e250cc41e/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f10bb0e4cd54f838e4213d12561dbe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/28 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "# Specify the path to your file\n",
    "path_to_file = \"final-space.jsonl\"\n",
    "\n",
    "# Load the dataset\n",
    "data = load_dataset('json', data_files=path_to_file)\n",
    "data = data.map(lambda samples: tokenizer(samples[\"text\"]), batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "183a9625-235e-4b03-9ade-d88b2d89e16c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d8156b3-5e6a-4ee6-b118-db3425efbd2f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 01:37, Epoch 2/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.786700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.781700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.383200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.401600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.281100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.165700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.044000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.214800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.023600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.750300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10, training_loss=2.283282923698425, metrics={'train_runtime': 142.5303, 'train_samples_per_second': 0.561, 'train_steps_per_second': 0.07, 'total_flos': 102380047220736.0, 'train_loss': 2.283282923698425, 'epoch': 2.86})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=data[\"train\"],\n",
    "    args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=8,\n",
    "        warmup_steps=2,\n",
    "        max_steps=10,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=True,\n",
    "        logging_steps=1,\n",
    "        output_dir=\"outputs\",\n",
    "        optim=\"paged_adamw_8bit\"\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a253815-b363-4996-878b-858a27286db5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2198531042d64fc0afc8822c055d5ebe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.bin:   0%|          | 0.00/34.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/FarziBuilder/NeoXAdapter/commit/dd9b10cca7fa090708f515ff7cfc8bf4dd879c26', commit_message='Upload model', commit_description='', oid='dd9b10cca7fa090708f515ff7cfc8bf4dd879c26', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(\"NeoXAdapter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "445e433e-6f79-4565-bda8-99780a06f1d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(\"myAdapter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1901ecaf-42bd-4740-b995-93701efdbfc3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_save = trainer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11de0023-6bc4-4dfe-bcde-89759e20c823",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lora_config = LoraConfig.from_pretrained('myAdapter')\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13aaaf46-2edd-4ba4-885d-ee1b75e3a404",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(\"perhapsModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1dd2b6a7-44fa-4dd9-af3d-a7502f5e312d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\n",
      "CUDA SETUP: CUDA runtime path found: /home/ec2-user/anaconda3/envs/pytorch_p310/lib/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 7.5\n",
      "CUDA SETUP: Detected CUDA version 118\n",
      "CUDA SETUP: Loading binary /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftConfig, PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab60058a-5682-4022-b171-d9373d0ce2e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12c4a27efdee4758beca2987f10a87ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "folder_name = \"myAdapter\"\n",
    "peft_config = PeftConfig.from_pretrained(folder_name)\n",
    "\n",
    "# Provide the offload_dir path\n",
    "offload_dir = \"directory\"  # replace with your actual directory path\n",
    "\n",
    "# Check if the directory exists, if not create it\n",
    "import os\n",
    "if not os.path.exists(offload_dir):\n",
    "    os.makedirs(offload_dir)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name, torch_dtype=torch.float16, load_in_8bit=False, device_map=\"auto\", trust_remote_code=True)\n",
    "#model = AutoModelForCausalLM.from_pretrained(\n",
    "#    model_name, device_map=\"auto\", offload_folder=\"offload\", offload_state_dict = True, torch_dtype=torch.float16\n",
    "#)\n",
    "#check_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "#check_model = AutoModelForCausalLM.from_pretrained(\n",
    "#    peft_config.base_model_name_or_path,\n",
    "#    return_dict=True,\n",
    "#    torch_dtype=torch.float16,\n",
    "#    low_cpu_mem_usage=True,\n",
    "#    trust_remote_code = True\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c40bb678-1398-4663-ab04-5a47da315498",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PEFT model loaded successfully.\n",
      "Merging LoRA and base model...\n"
     ]
    }
   ],
   "source": [
    "folder_name = \"myAdapter\"\n",
    "check_model = PeftModel.from_pretrained(model, folder_name)\n",
    "check_model.eval()\n",
    "\n",
    "print(\"PEFT model loaded successfully.\")\n",
    "\n",
    "# Merge LoRA and base model and save\n",
    "print(\"Merging LoRA and base model...\")\n",
    "merged_model = check_model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ed3cb6e5-65d4-49e6-a9a5-d1a08edf1e17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged_model.save_pretrained(\"myLLaMa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bed72f4a-2e01-42ae-8721-b9c1100a9d14",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6456fb67db9b4868b41d8cf2e36c7228",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/667M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/FarziBuilder/fastInferencetry9/commit/a4b37544e4f7cce62eed452a5a1abacc53265d78', commit_message='Upload LlamaForCausalLM', commit_description='', oid='a4b37544e4f7cce62eed452a5a1abacc53265d78', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_model.push_to_hub(\"fastInferencetry9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120d142f-0f55-4425-8a1d-6b2348d79de3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged_model.save_pretrained(\"savedModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5195182a-5a46-45c7-b99d-cff5effcb1bc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/FarziBuilder/fastInferencetry9/commit/519692aa33ae821b8a70e01abc2de9b168aaa5fc', commit_message='Upload tokenizer', commit_description='', oid='519692aa33ae821b8a70e01abc2de9b168aaa5fc', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.push_to_hub(\"fastInferencetry9\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bdf96d50-4a84-4c41-8fcb-7f4a0037b0d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who is the PM of UK?(Answer:Rishi Sunak is the Prime Minister of UK)\n",
      "Who is the PM of UK?\n",
      "Rishi Sunak is the Prime Minister of UK.\n",
      "Rishi Sunak is the 30th and current Prime Minister of the United Kingdom. He is the Member of Parliament for Richmond (Yorks) and the Chief Secretary to the Treasury.\n",
      "Rishi Sunak is the 30th and current Prime Minister of the United Kingdom. He is the Member of Parliament for Richmond (Yorks) and the Chief Secretary\n"
     ]
    }
   ],
   "source": [
    "text = \"Who is the PM of UK?(Answer:Rishi Sunak is the Prime Minister of UK)\"\n",
    "device = \"cuda:0\"\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Ensure 'attention_mask' and 'pad_token' are returned.\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", padding='longest', truncation=True).to(device)\n",
    "\n",
    "# Explicitly set the pad_token_id\n",
    "\n",
    "\n",
    "outputs = merged_model.generate(input_ids=inputs['input_ids'], \n",
    "                         attention_mask=inputs['attention_mask'], \n",
    "                         pad_token_id=tokenizer.eos_token_id, \n",
    "                         max_new_tokens=100)\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8766a209-9efb-48ec-a864-dc3cc715af43",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22d4e850a37147eaaf181a0257dddf8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0d6eb6d70454cb8b6c5f594f8bb24ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00002-of-00002.bin:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4a40083c18b4355860faaf00f9eccab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00001-of-00002.bin:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/FarziBuilder/fastInferencetry10/commit/43c4b1b045f64bfe0195131297cc90168b6b5489', commit_message='Upload LlamaForCausalLM', commit_description='', oid='43c4b1b045f64bfe0195131297cc90168b6b5489', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_model.push_to_hub(\"fastInferencetry10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f471db6-484c-4669-be18-ff4a7659374b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/FarziBuilder/fastInferencetry10/commit/2459ab079efe3725cd1032cb695f5020cbd50fb7', commit_message='Upload tokenizer', commit_description='', oid='2459ab079efe3725cd1032cb695f5020cbd50fb7', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.push_to_hub(\"fastInferencetry10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6b9e60-d366-4251-a7dc-5d99e438dbe1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
